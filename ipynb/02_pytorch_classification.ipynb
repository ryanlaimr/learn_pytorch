{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 02. PyTorch Neural Network Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd68278d6dc9b56f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Architecture of a classification neural network\n",
    "\n",
    "| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n",
    "| --- | --- | --- |\n",
    "| **Input layer shape** (`in_features`) | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n",
    "| **Hidden layer(s)** | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n",
    "| **Neurons per hidden layer** | Problem specific, generally 10 to 512 | Same as binary classification |\n",
    "| **Output layer shape** (`out_features`) | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n",
    "| **Hidden layer activation** | Usually [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) (rectified linear unit) but [can be many others](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) | Same as binary classification |\n",
    "| **Output activation** | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) ([`torch.sigmoid`](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) in PyTorch)| [Softmax](https://en.wikipedia.org/wiki/Softmax_function) ([`torch.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) in PyTorch) |\n",
    "| **Loss function** | [Binary crossentropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) in PyTorch) | Cross entropy ([`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in PyTorch) |\n",
    "| **Optimizer** | [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent), [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) (see [`torch.optim`](https://pytorch.org/docs/stable/optim.html) for more options) | Same as binary classification |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2505daa37f734167"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Make classification data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "255c2df776fae98c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Build a model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc5265af8157c12"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'mps'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T02:21:13.641268Z",
     "start_time": "2024-01-27T02:21:11.530376Z"
    }
   },
   "id": "946192f7638d00ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can use `nn.Sequential` to make the code easier to understand."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334d867633157f5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T02:22:57.510237Z",
     "start_time": "2024-01-27T02:22:57.225886Z"
    }
   },
   "id": "160fbc94891ba2e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "| Loss function/Optimizer | Problem type | PyTorch Code |\n",
    "| ----- | ----- | ----- |\n",
    "| Stochastic Gradient Descent (SGD) optimizer | Classification, regression, many others. | [`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) |\n",
    "| Adam Optimizer | Classification, regression, many others. | [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) |\n",
    "| Binary cross entropy loss | Binary classification | [`torch.nn.BCELossWithLogits`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) or [`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) |\n",
    "| Cross entropy loss | Mutli-class classification | [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) |\n",
    "| Mean absolute error (MAE) or L1 Loss | Regression | [`torch.nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) | \n",
    "| Mean squared error (MSE) or L2 Loss | Regression | [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) |  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8622f29aac57ad71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Train model\n",
    "## 4. Make predictions and evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e00c52983d385f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Improving a model (from a model perspective) \n",
    "\n",
    "| Model improvement technique* | What does it do? |\n",
    "| ----- | ----- |\n",
    "| **Add more layers** | Each layer *potentially* increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network *deeper*. |\n",
    "| **Add more hidden units** | Similar to the above, more hidden units per layer means a *potential* increase in learning capabilities of the model, more hidden units is often referred to as making your neural network *wider*. |\n",
    "| **Fitting for longer (more epochs)** | Your model might learn more if it had more opportunities to look at the data. |\n",
    "| **Changing the activation functions** | Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). |\n",
    "| **Change the learning rate** | Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. |\n",
    "| **Change the loss function** | Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. |\n",
    "| **Use transfer learning** | Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in [notebook 06](https://www.learnpytorch.io/06_pytorch_transfer_learning/). |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9470fc0f8f33b90f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
