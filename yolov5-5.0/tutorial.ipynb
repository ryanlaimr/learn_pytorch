{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "b54ab52f1d4f4903897ab6cd49a3b9b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_1852f93fc2714d40adccb8aa161c42ff",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_3293cfe869bd4a1bbbe18b49b6815de1",
       "IPY_MODEL_8d5ee8b8ab6d46b98818bd2c562ddd1c"
      ]
     }
    },
    "1852f93fc2714d40adccb8aa161c42ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3293cfe869bd4a1bbbe18b49b6815de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_49fcb2adb0354430b76f491af98abfe9",
      "_dom_classes": [],
      "description": "100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 819257867,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 819257867,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_c7d76e0c53064363add56b8d05e561f5"
     }
    },
    "8d5ee8b8ab6d46b98818bd2c562ddd1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_48f321f789634aa584f8a29a3b925dd5",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 781M/781M [00:13&lt;00:00, 62.6MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6610d6275f3e49d9937d50ed0a105947"
     }
    },
    "49fcb2adb0354430b76f491af98abfe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "c7d76e0c53064363add56b8d05e561f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "48f321f789634aa584f8a29a3b925dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "6610d6275f3e49d9937d50ed0a105947": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "0fffa335322b41658508e06aed0acbf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_a354c6f80ce347e5a3ef64af87c0eccb",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_85823e71fea54c39bd11e2e972348836",
       "IPY_MODEL_fb11acd663fa4e71b041d67310d045fd"
      ]
     }
    },
    "a354c6f80ce347e5a3ef64af87c0eccb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "85823e71fea54c39bd11e2e972348836": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_8a919053b780449aae5523658ad611fa",
      "_dom_classes": [],
      "description": "100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 22091032,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 22091032,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_5bae9393a58b44f7b69fb04816f94f6f"
     }
    },
    "fb11acd663fa4e71b041d67310d045fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_d26c6d16c7f24030ab2da5285bf198ee",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 21.1M/21.1M [00:02&lt;00:00, 9.36MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_f7767886b2364c8d9efdc79e175ad8eb"
     }
    },
    "8a919053b780449aae5523658ad611fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "5bae9393a58b44f7b69fb04816f94f6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "d26c6d16c7f24030ab2da5285bf198ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "f7767886b2364c8d9efdc79e175ad8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvhYZrIZCEyo"
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/26833433/98702494-b71c4e80-237a-11eb-87ed-17fcd6b3f066.jpg\">\n",
    "\n",
    "This notebook was written by Ultralytics LLC, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
    "For more information please visit https://github.com/ultralytics/yolov5 and https://www.ultralytics.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone repo, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wbvMlHd_QwMG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20027455-bf84-41fd-c902-b7282d53c91d"
   },
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "#%cd yolov5\n",
    "#%pip install -qr requirements.txt  # install dependencies\n",
    "\n",
    "import torch\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "clear_output()\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'mps'))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 2.1.2 mps\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Inference\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/114307955-5c7e4e80-9ae2-11eb-9f50-a90e39bee53f.png\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zR9ZbuQCH7FX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "outputId": "c9a308f7-2216-4805-8003-eca8dd0dc30d",
    "ExecuteTime": {
     "end_time": "2024-01-28T06:53:36.733968Z",
     "start_time": "2024-01-28T06:53:02.797846Z"
    }
   },
   "source": [
    "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\n",
    "Image(filename='runs/detect/exp/zidane.jpg', width=600)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov5s.pt'], source='data/images/', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False)\r\n",
      "YOLOv5 ðŸš€ 1f1a8e6 torch 2.1.2 CPU\r\n",
      "\r\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:01<00:00, 12.9MB/s]\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/ryanlai/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/GitHubRepos/learn_pytorch/yolov5-5.0/detect.py\", line 178, in <module>\r\n",
      "    detect()\r\n",
      "  File \"/Users/ryanlai/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/GitHubRepos/learn_pytorch/yolov5-5.0/detect.py\", line 34, in detect\r\n",
      "    model = attempt_load(weights, map_location=device)  # load FP32 model\r\n",
      "  File \"/Users/ryanlai/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/GitHubRepos/learn_pytorch/yolov5-5.0/models/experimental.py\", line 118, in attempt_load\r\n",
      "    ckpt = torch.load(w, map_location=map_location)  # load\r\n",
      "  File \"/Users/ryanlai/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\", line 1014, in load\r\n",
      "    return _load(opened_zipfile,\r\n",
      "  File \"/Users/ryanlai/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\", line 1422, in _load\r\n",
      "    result = unpickler.load()\r\n",
      "  File \"/Users/ryanlai/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py\", line 1415, in find_class\r\n",
      "    return super().find_class(mod_name, name)\r\n",
      "AttributeError: Can't get attribute 'SPPF' on <module 'models.common' from '/Users/ryanlai/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/GitHubRepos/learn_pytorch/yolov5-5.0/models/common.py'>\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/detect/exp/zidane.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpython detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mImage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mruns/detect/exp/zidane.jpg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m600\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/IPython/core/display.py:970\u001B[0m, in \u001B[0;36mImage.__init__\u001B[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munconfined \u001B[38;5;241m=\u001B[39m unconfined\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malt \u001B[38;5;241m=\u001B[39m alt\n\u001B[0;32m--> 970\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mImage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m, {}):\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m=\u001B[39m metadata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/IPython/core/display.py:327\u001B[0m, in \u001B[0;36mDisplayObject.__init__\u001B[0;34m(self, data, url, filename, metadata)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 327\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_data()\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/IPython/core/display.py:1005\u001B[0m, in \u001B[0;36mImage.reload\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001B[39;00m\n\u001B[1;32m   1004\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed:\n\u001B[0;32m-> 1005\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mImage\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1006\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretina:\n\u001B[1;32m   1007\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retina_shape()\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/IPython/core/display.py:353\u001B[0m, in \u001B[0;36mDisplayObject.reload\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    352\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_flags \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 353\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_flags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    354\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;66;03m# Deferred import\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'runs/detect/exp/zidane.jpg'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Test\n",
    "Test a model on [COCO](https://cocodataset.org/#home) val or test-dev dataset to evaluate trained accuracy. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be 1-2% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyTZYGgRjnMc"
   },
   "source": [
    "## COCO val2017\n",
    "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQPtK1QYVaD_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "b54ab52f1d4f4903897ab6cd49a3b9b2",
      "1852f93fc2714d40adccb8aa161c42ff",
      "3293cfe869bd4a1bbbe18b49b6815de1",
      "8d5ee8b8ab6d46b98818bd2c562ddd1c",
      "49fcb2adb0354430b76f491af98abfe9",
      "c7d76e0c53064363add56b8d05e561f5",
      "48f321f789634aa584f8a29a3b925dd5",
      "6610d6275f3e49d9937d50ed0a105947"
     ]
    },
    "outputId": "f0884441-78d9-443c-afa6-d00ec387908d",
    "ExecuteTime": {
     "end_time": "2024-01-28T06:55:10.982376Z",
     "start_time": "2024-01-28T06:53:49.892605Z"
    }
   },
   "source": [
    "# Download COCO val2017\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780M/780M [00:59<00:00, 13.7MB/s] \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X58w8JLpMnjH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5b54c11e-9f4b-4d9a-8e6e-6a2a4f0cc60d"
   },
   "source": [
    "# Run YOLOv5x on COCO val2017\n",
    "!python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', exist_ok=False, img_size=640, iou_thres=0.65, name='exp', project='runs/test', save_conf=False, save_hybrid=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt'])\n",
      "YOLOv5 ðŸš€ v4.0-137-g9b11f0c torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5x.pt to yolov5x.pt...\n",
      "100% 168M/168M [00:02<00:00, 59.1MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning '../coco/val2017' for images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 3236.68it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: ../coco/val2017.cache\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 157/157 [01:20<00:00,  1.95it/s]\n",
      "                 all        5000       36335       0.749       0.619        0.68       0.486\n",
      "Speed: 5.3/1.7/6.9 ms inference/NMS/total per 640x640 image at batch-size 32\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=5.10s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=88.52s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=17.17s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.687\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.544\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.338\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\n",
      "Results saved to runs/test/exp\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc_KbFk0juX2"
   },
   "source": [
    "## COCO test-dev2017\n",
    "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V0AJnSeCIHyJ"
   },
   "source": [
    "# Download COCO test-dev2017\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip # unzip labels\n",
    "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n",
    "%mv ./test2017 ../coco/images  # move to /coco"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29GJXAP_lPrt"
   },
   "source": [
    "# Run YOLOv5s on COCO test-dev2017 using --task test\n",
    "!python test.py --weights yolov5s.pt --data coco.yaml --task test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOiNLtMP5aG"
   },
   "source": [
    "# 3. Train\n",
    "\n",
    "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Knxi2ncxWffW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "0fffa335322b41658508e06aed0acbf0",
      "a354c6f80ce347e5a3ef64af87c0eccb",
      "85823e71fea54c39bd11e2e972348836",
      "fb11acd663fa4e71b041d67310d045fd",
      "8a919053b780449aae5523658ad611fa",
      "5bae9393a58b44f7b69fb04816f94f6f",
      "d26c6d16c7f24030ab2da5285bf198ee",
      "f7767886b2364c8d9efdc79e175ad8eb"
     ]
    },
    "outputId": "b41ac253-9e1b-4c26-d78b-700ea0154f43"
   },
   "source": [
    "# Download COCO128\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fffa335322b41658508e06aed0acbf0",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22091032.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pOkGLv1dMqh"
   },
   "source": [
    "Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
    "\n",
    "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bOy5KI2ncnWd"
   },
   "source": [
    "# Tensorboard (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2fLAV42oNb7M"
   },
   "source": [
    "# Weights & Biases (optional)\n",
    "%pip install -q wandb  \n",
    "!wandb login  # use 'wandb disabled' or 'wandb enabled' to disable or enable"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1NcFxRcFdJ_O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf494627-09b9-4399-ff0c-fdb62b32340a"
   },
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mgithub: \u001B[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v4.0-137-g9b11f0c torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
      "\n",
      "Namespace(adam=False, batch_size=16, bucket='', cache_images=True, cfg='', data='./data/coco128.yaml', device='', entity=None, epochs=3, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], linear_lr=False, local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='exp', noautoanchor=False, nosave=True, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp', single_cls=False, sync_bn=False, total_batch_size=16, weights='yolov5s.pt', workers=8, world_size=1)\n",
      "\u001B[34m\u001B[1mwandb: \u001B[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "2021-03-14 04:18:58.124672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to yolov5s.pt...\n",
      "100% 14.1M/14.1M [00:00<00:00, 63.1MB/s]\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n",
      "\n",
      "Transferred 362/362 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning '../coco128/labels/train2017' for images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 2956.76it/s]\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: ../coco128/labels/train2017.cache\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 205.30it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning '../coco128/labels/train2017.cache' for images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 604584.36it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mCaching images (0.1GB): 100% 128/128 [00:00<00:00, 144.17it/s]\n",
      "Plotting labels... \n",
      "\n",
      "\u001B[34m\u001B[1mautoanchor: \u001B[0mAnalyzing anchors... anchors/target = 4.26, Best Possible Recall (BPR) = 0.9946\n",
      "Image sizes 640 train, 640 test\n",
      "Using 2 dataloader workers\n",
      "Logging results to runs/train/exp\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "       0/2     3.29G   0.04237   0.06417   0.02121    0.1277       183       640: 100% 8/8 [00:03<00:00,  2.41it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:04<00:00,  1.04s/it]\n",
      "                 all         128         929       0.642       0.637       0.661       0.432\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "       1/2     6.65G   0.04431   0.06403     0.019    0.1273       166       640: 100% 8/8 [00:01<00:00,  5.73it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:01<00:00,  3.21it/s]\n",
      "                 all         128         929       0.662       0.626       0.658       0.433\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "       2/2     6.65G   0.04506   0.06836   0.01913    0.1325       182       640: 100% 8/8 [00:01<00:00,  5.51it/s]\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.35it/s]\n",
      "                 all         128         929       0.658       0.625       0.661       0.433\n",
      "Optimizer stripped from runs/train/exp/weights/last.pt, 14.8MB\n",
      "Optimizer stripped from runs/train/exp/weights/best.pt, 14.8MB\n",
      "3 epochs completed in 0.007 hours.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15glLzbQx5u0"
   },
   "source": [
    "# 4. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "## Weights & Biases Logging ðŸŒŸ NEW\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
    "\n",
    "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WPvRbS5Swl6"
   },
   "source": [
    "## Local Logging\n",
    "\n",
    "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "riPdhraOTCO0"
   },
   "source": [
    "Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n",
    "Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # test batch 0 labels\n",
    "Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # test batch 0 predictions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYG4WFEnTVrI"
   },
   "source": [
    "> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"750\">  \n",
    "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"750\">  \n",
    "`test_batch0_labels.jpg` shows test batch 0 labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"750\">  \n",
    "`test_batch0_pred.jpg` shows test batch 0 _predictions_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KN5ghjE6ZWh"
   },
   "source": [
    "Training losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov5s.pt` (orange)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MDznIqPF7nk3"
   },
   "source": [
    "from utils.plots import plot_results \n",
    "plot_results(save_dir='runs/train/exp')  # plot all results*.txt as results.png\n",
    "Image(filename='runs/train/exp/results.png', width=800)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfrEegCSW3fK"
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/26833433/97808309-8182b180-1c66-11eb-8461-bffe1a79511d.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zelyeqbyt3GD"
   },
   "source": [
    "# Environments\n",
    "\n",
    "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
    "\n",
    "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
    "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
    "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
    "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qu7Iesl0p54"
   },
   "source": [
    "# Status\n",
    "\n",
    "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
    "\n",
    "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov5/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEijrePND_2I"
   },
   "source": [
    "# Appendix\n",
    "\n",
    "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gI6NoBev8Ib1"
   },
   "source": [
    "# Re-clone repo\n",
    "%cd ..\n",
    "%rm -rf yolov5 && git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mcKoSIK2WSzj"
   },
   "source": [
    "# Reproduce\n",
    "for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n",
    "  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n",
    "  !python test.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FGH0ZjkGjejy"
   },
   "source": [
    "# Unit tests\n",
    "%%shell\n",
    "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
    "\n",
    "rm -rf runs  # remove runs/\n",
    "for m in yolov5s; do  # models\n",
    "  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n",
    "  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n",
    "  for d in 0 cpu; do  # devices\n",
    "    python detect.py --weights $m.pt --device $d  # detect official\n",
    "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
    "    python test.py --weights $m.pt --device $d # test official\n",
    "    python test.py --weights runs/train/exp/weights/best.pt --device $d # test custom\n",
    "  done\n",
    "  python hubconf.py  # hub\n",
    "  python models/yolo.py --cfg $m.yaml  # inspect\n",
    "  python models/export.py --weights $m.pt --img 640 --batch 1  # export\n",
    "done"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gogI-kwi3Tye"
   },
   "source": [
    "# Profile\n",
    "from utils.torch_utils import profile \n",
    "\n",
    "m1 = lambda x: x * torch.sigmoid(x)\n",
    "m2 = torch.nn.SiLU()\n",
    "profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RVRSOhEvUdb5"
   },
   "source": [
    "# Evolve\n",
    "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
    "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BSgFCAcMbk1R"
   },
   "source": [
    "# VOC\n",
    "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
    "  !python train.py --batch {b} --weights {m}.pt --data voc.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
